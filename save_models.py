"""
Ø³ÙƒØ±ÙŠØ¨Øª Ù„Ø­ÙØ¸ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ ÙˆØ§Ù„Ù…Ø¹Ø§Ù„Ø¬ Ø§Ù„Ù…Ø³Ø¨Ù‚ ÙˆØ¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±
ÙŠØ¬Ø¨ ØªØ´ØºÙŠÙ„ Ù‡Ø°Ø§ Ø§Ù„Ø³ÙƒØ±ÙŠØ¨Øª Ù…Ù† Ø¯Ø§Ø®Ù„ notebook Ø¨Ø¹Ø¯ ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬
"""

import joblib
import numpy as np
import pandas as pd
import os

print("=" * 60)
print("Ø¨Ø¯Ø¡ Ø¹Ù…Ù„ÙŠØ© Ø­ÙØ¸ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ ÙˆØ§Ù„Ø¨ÙŠØ§Ù†Ø§Øª...")
print("=" * 60)

# Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ø§Ù„Ù…ØªØºÙŠØ±Ø§Øª Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©
required_vars = {
    'preprocessor': 'preprocessor',
    'lr': 'lr',
    'rf': 'rf',
    'hgb': 'hgb',
    'w_lr': 'w_lr',
    'w_rf': 'w_rf',
    'w_hgb': 'w_hgb',
    't_ens': 't_ens',
    'X_test': 'X_test',
    'y_test': 'y_test',
    'ACTIVE_FEATURES': 'ACTIVE_FEATURES'
}

missing_vars = []
for var_name, var_display in required_vars.items():
    if var_name not in globals():
        missing_vars.append(var_display)

if missing_vars:
    print("\nâŒ Ø®Ø·Ø£: Ø§Ù„Ù…ØªØºÙŠØ±Ø§Øª Ø§Ù„ØªØ§Ù„ÙŠØ© ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯Ø©:")
    for var in missing_vars:
        print(f"   - {var}")
    print("\nâš ï¸ ÙŠØ±Ø¬Ù‰ Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù†:")
    print("   1. ØªØ´ØºÙŠÙ„ Ø¬Ù…ÙŠØ¹ Ø®Ù„Ø§ÙŠØ§ Notebook")
    print("   2. ØªØ¯Ø±ÙŠØ¨ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬")
    print("   3. ØªØ´ØºÙŠÙ„ Ø®Ù„Ø§ÙŠØ§ Ø§Ù„ØªÙ‚ÙŠÙŠÙ…")
    raise NameError("Ø¨Ø¹Ø¶ Ø§Ù„Ù…ØªØºÙŠØ±Ø§Øª Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯Ø©")

print("\nâœ“ ØªÙ… Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…ØªØºÙŠØ±Ø§Øª Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©\n")

try:
    # Ø­ÙØ¸ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬ Ø§Ù„Ù…Ø³Ø¨Ù‚
    joblib.dump(preprocessor, 'preprocessor.pkl')
    print("âœ“ ØªÙ… Ø­ÙØ¸ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬ Ø§Ù„Ù…Ø³Ø¨Ù‚ (preprocessor.pkl)")
    
    # Ø­ÙØ¸ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬
    joblib.dump(lr, 'logistic_regression.pkl')
    print("âœ“ ØªÙ… Ø­ÙØ¸ Logistic Regression (logistic_regression.pkl)")
    
    joblib.dump(rf, 'random_forest.pkl')
    print("âœ“ ØªÙ… Ø­ÙØ¸ Random Forest (random_forest.pkl)")
    
    joblib.dump(hgb, 'hist_gradient_boosting.pkl')
    print("âœ“ ØªÙ… Ø­ÙØ¸ HistGradientBoosting (hist_gradient_boosting.pkl)")
    
    # Ø­ÙØ¸ Ø§Ù„Ø£ÙˆØ²Ø§Ù† ÙˆØ§Ù„Ø¹ØªØ¨Ø© Ù„Ù„Ù€ Ensemble
    ensemble_config = {
        'w_lr': w_lr,
        'w_rf': w_rf,
        'w_hgb': w_hgb,
        'threshold': t_ens
    }
    joblib.dump(ensemble_config, 'ensemble_config.pkl')
    print("âœ“ ØªÙ… Ø­ÙØ¸ Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ensemble (ensemble_config.pkl)")
    
    # Ø­ÙØ¸ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø± (Ø¹ÙŠÙ†Ø© ØµØºÙŠØ±Ø© Ù„ØªÙˆÙÙŠØ± Ø§Ù„Ø°Ø§ÙƒØ±Ø©)
    # Ø­ÙØ¸ 10000 Ø¹ÙŠÙ†Ø© Ø¹Ø´ÙˆØ§Ø¦ÙŠØ© Ù…Ù† Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±
    np.random.seed(42)  # Ù„Ù„ØªÙƒØ±Ø§Ø±
    test_sample_size = min(10000, len(X_test))
    test_indices = np.random.choice(len(X_test), test_sample_size, replace=False)
    
    if hasattr(X_test, 'iloc'):
        test_data = {
            'X_test': X_test.iloc[test_indices].copy(),
            'y_test': y_test.iloc[test_indices].copy() if hasattr(y_test, 'iloc') else y_test[test_indices]
        }
    else:
        test_data = {
            'X_test': X_test[test_indices].copy(),
            'y_test': y_test[test_indices].copy()
        }
    
    joblib.dump(test_data, 'test_data.pkl')
    print(f"âœ“ ØªÙ… Ø­ÙØ¸ {test_sample_size} Ø¹ÙŠÙ†Ø© Ù…Ù† Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø± (test_data.pkl)")
    
    # Ø­ÙØ¸ Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù†Ø´Ø·Ø©
    joblib.dump(ACTIVE_FEATURES, 'active_features.pkl')
    print("âœ“ ØªÙ… Ø­ÙØ¸ Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù†Ø´Ø·Ø© (active_features.pkl)")
    
    print("\n" + "=" * 60)
    print("âœ… ØªÙ… Ø­ÙØ¸ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ù„ÙØ§Øª Ø¨Ù†Ø¬Ø§Ø­!")
    print("=" * 60)
    
    # Ø¹Ø±Ø¶ Ø­Ø¬Ù… Ø§Ù„Ù…Ù„ÙØ§Øª
    print("\nğŸ“Š Ø­Ø¬Ù… Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…Ø­ÙÙˆØ¸Ø©:")
    files = [
        'preprocessor.pkl',
        'logistic_regression.pkl',
        'random_forest.pkl',
        'hist_gradient_boosting.pkl',
        'ensemble_config.pkl',
        'test_data.pkl',
        'active_features.pkl'
    ]
    
    total_size = 0
    for file in files:
        if os.path.exists(file):
            size = os.path.getsize(file) / (1024 * 1024)  # Ø¨Ø§Ù„Ù…ÙŠØ¬Ø§Ø¨Ø§ÙŠØª
            total_size += size
            print(f"   {file}: {size:.2f} MB")
    
    print(f"\n   Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ø­Ø¬Ù…: {total_size:.2f} MB")
    print("\nâœ… ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ù„Ø¢Ù† ØªØ´ØºÙŠÙ„: streamlit run app.py")
    
except Exception as e:
    print(f"\nâŒ Ø­Ø¯Ø« Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„Ø­ÙØ¸: {str(e)}")
    print("\nâš ï¸ ÙŠØ±Ø¬Ù‰ Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù†:")
    print("   1. ÙˆØ¬ÙˆØ¯ Ù…Ø³Ø§Ø­Ø© ÙƒØ§ÙÙŠØ© Ø¹Ù„Ù‰ Ø§Ù„Ù‚Ø±Øµ")
    print("   2. ØµÙ„Ø§Ø­ÙŠØ§Øª Ø§Ù„ÙƒØªØ§Ø¨Ø© ÙÙŠ Ø§Ù„Ù…Ø¬Ù„Ø¯ Ø§Ù„Ø­Ø§Ù„ÙŠ")
    raise

